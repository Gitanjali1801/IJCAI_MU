# -*- coding: utf-8 -*-
"""baseline_machine_unlearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kDf5XPxNbxNUufSZNZEUe3qhlDrYK9R5
"""

# Commented out IPython magic to ensure Python compatibility.
# %export CUBLAS_WORKSPACE_CONFIG=:4096:8

import warnings
import os
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = '3'
# import os
os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'

# !pip install git+https://github.com/openai/CLIP.git

import torch
import clip
# !pip install git+https://github.com/openai/CLIP.git
device = 'cuda' if torch.cuda.is_available() else 'cpu'
clip_model, compose = clip.load('ViT-L/14', device = device)
# text_model = text_model.cpu()
def process(idx_val,arr):
  if idx_val=='0':
    arr.append(0)
  else:
    arr.append(1)



# load models and model components
frcnn_cfg = utils.Config.from_pretrained("unc-nlp/frcnn-vg-finetuned")

frcnn = GeneralizedRCNN.from_pretrained("unc-nlp/frcnn-vg-finetuned", config=frcnn_cfg)

image_preprocess = Preprocess(frcnn_cfg)

# bert_tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
visualbert_vqa = VisualBertForQuestionAnswering.from_pretrained("uclanlp/visualbert-vqa")

img_paths = []

img_paths

img_paths = []

def get_img_embedding(img_paths):
    # img_paths.append('../../data/img/'+id2text[meme][2])
	images, sizes, scales_yx = image_preprocess(img_paths) # img_paths -> list of image paths
	# output_dict = frcnn(
	#   images,
	#   sizes,
	#   scales_yx=scales_yx,
	#   padding="max_detections",
	#   max_detections=frcnn_cfg.max_detections,
	#   return_tensors="pt",
	# )
	output_dict = frcnn(
    images,
    sizes,
    scales_yx=scales_yx,
    padding="max_detections",
    max_detections=frcnn_cfg.max_detections,
    return_tensors="pt",)
	features = output_dict.get("roi_features")
	visual_embeds = features
	visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)
	visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)
	return visual_embeds,visual_token_type_ids,visual_attention_mask

visual_feats = {}

# exit(0)

for index in tqdm(range(len(data))):

    try:
        visual_embeds,visual_token_type_ids,visual_attention_mask = get_img_embedding(str('path/gender_biasness/MAMI_2022_images/training_images/'+data.iloc[index]['file_name']))
    except:
        print(str(data.iloc[index]['file_name']))
        visual_embeds,visual_token_type_ids,visual_attention_mask = get_img_embedding(str(data.iloc[22]['file_name']))

    visual_feats[str(data.iloc[index]['file_name'])] = (visual_embeds, visual_token_type_ids, visual_attention_mask)


# torch.save(visual_feats, './vs_tensors_file_name_train.pt')

!pwd

torch.save(visual_feats, './vs_tensors_MAMI_train.pt')

# Commented out IPython magic to ensure Python compatibility.
# %cd pathnaacl_2023

from transformers import BertTokenizer, VisualBertForPreTraining
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

import torch
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler

# vis_feats = torch.load('pathnaacl_2023/vs_tensors_MAMI_test.pt')
vis_feats = torch.load('pathnaacl_2023/vs_tensors_MAMI_train.pt')

# keywords_list=list(data_train["List"])

# for i in range(len(keywords_list)):
#     query_embedding = model.encode(keywords_list[i])
# sim_final, similarity=[],[]
# for i in range(len(keywords_list)):
#     query_embedding = model.encode(keywords_list[i])
# #The passages are encoded as [ [title1, text1], [title2, text2], ...]
# # passage_embedding = model.encode([['Beef ban', 'kashmirisbleeding']])
#     for j in range(len(keywords_list)):
#       passage_embedding = model.encode([keywords_list[j]])
#       # print("Similarity:", util.cos_sim(query_embedding, passage_embedding))
#       sim=util.cos_sim(query_embedding, passage_embedding)
#       sim_new=sim.numpy()
#       similarity.append(sim_new)
#     sim_final.append(similarity)

# sim_final, similarity=[],[]
# for i in range(len(keywords_list)):
#     query_embedding = model.encode(keywords_list[i])
# #The passages are encoded as [ [title1, text1], [title2, text2], ...]
# # passage_embedding = model.encode([['Beef ban', 'kashmirisbleeding']])
#     # for j in range(len(keywords_list)):
#     passage_embedding = model.encode(keywords_list)
#     # print("Similarity:", util.cos_sim(query_embedding, passage_embedding))
#     sim=util.cos_sim(query_embedding, passage_embedding)
#     sim_new=sim.numpy()
#     # print(sim_new)
#     similarity.append(sim_new)
# sim_final.append(similarity)

# sim_new

# similarity

# sim_final_new=sim_final[5].numpy()

# sim_final[1]

# sim_final[2][2]

# sim_final_new=sim_final.numpy()

# actual_label=np.load("path/different_biasness_codes/code_try/final_code_attention_clip/actual_label_offensive_after_debias.npy")
# actual_label_before_explain=np.load("path/explainable_model/predicted_offensive_after_debias.npy")
# predicted_label=np.load("path/different_biasness_codes/code_try/final_code_attention_clip/predicted_offensive_after_debias.npy")
# name=np.load("path//different_biasness_codes/code_try/final_code_attention_clip/name_after_debias.npy")
# list(name)
# list(actual_label)
# list(predicted_label)
# df_test = pd.DataFrame(columns=['name','similarity'])
# df_test['name'] = keywords_list
# df_test['similarity'] = similarity
# # df_test['predicted_label'] = predicted_label
# # df_test['img_grad'] = img_grad
# # df_test['txt_grad'] = text_grad
# # df_test.to_csv('unbiased_text_replacement_baseline_result_multimodal' + ".csv", sep=',')
# df_test.to_csv('list_with_similarity' + ".csv", sep=',')

# query_embedding = model.encode('प्यार')
# #The passages are encoded as [ [title1, text1], [title2, text2], ...]
# passage_embedding = model.encode(['नफरत'])
# print("Similarity:", util.cos_sim(query_embedding, passage_embedding))
# sim= util.cos_sim(query_embedding, passage_embedding)
# sim_test=sim.numpy()
# print(sim_test)

# sentences = ['Beef ban',
#              'mimi chakraborty']


#              	# Nusrat jahan , mimi chakraborty]

# #Compute embeddings
# embeddings = model.encode(sentences, convert_to_tensor=True)

# #Compute cosine-similarities for each sentence with each other sentence
# cosine_scores = util.cos_sim(embeddings, embeddings)

# cosine_scores

#!cp -r /content/drive/.shortcut-targets-by-id/1Z57L19m3ZpJ6bEPdyaIMYuI00Tc2RT1I/memes_our_dataset_hindi /content/

!pwd

import pandas as pd
import numpy as np

data = pd.read_csv('path/gender_biasness/MAMI_train_scene_graph.csv')
data_test = pd.read_csv('path/gender_biasness/MAMI_test_scene_graph.csv')

data

from collections import Counter

def analyze_gender_bias(meme_objects, labels):
    # Create a counter to keep track of gender-related objects
    gender_counter = Counter()

    # Iterate over each meme sample
    for objects, label in zip(meme_objects, labels):
        if label == "misogynous":
            # Increment the counter for gender-related objects
            gender_counter.update(objects)

    # Calculate the total count of gender-related objects
    total_count = sum(gender_counter.values())

    # Calculate the bias score
    bias_score = sum(gender_counter.values()) / len(meme_objects)

    # Calculate the percentage distribution of gender-related objects
    gender_distribution = {object_name: count / total_count * 100 for object_name, count in gender_counter.items()}

    return bias_score, gender_distribution

# Example usage
meme_objects = [
    ["woman", "man", "cat", "dog"],
    ["man", "car", "tree"],
    ["woman", "flower", "book", "man"],
]
labels = ["misogynous", "non-misogynous", "misogynous"]

bias_score, gender_distribution = analyze_gender_bias(meme_objects, labels)

print("Bias Score:", bias_score)
print("Gender Distribution:", gender_distribution)

data_test

data.head(10)

import os
import torch
import pandas as pd
from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from tqdm.notebook import tqdm
import clip
from PIL import Image

model, preprocess = clip.load("ViT-B/32")

# !pip install multilingual-clip

from multilingual_clip import pt_multilingual_clip
import transformers

texts = [
    'Three blind horses listening to Mozart.',
    'Älgen är skogens konung!',
    'Wie leben Eisbären in der Antarktis?',
    'Вы знали, что все белые медведи левши?'
]
model_name = 'M-CLIP/XLM-Roberta-Large-Vit-L-14'
# Load Model & Tokenizer
model = pt_multilingual_clip.MultilingualCLIP.from_pretrained(model_name)
tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)
embeddings = model.forward(texts, tokenizer)
print(embeddings.shape)

sample = data['Text_Transcription'][10]

sample

image = preprocess(Image.open("PATH/my_roman_actual/eng_file134.png")).unsqueeze(0).to(device)
text = clip.tokenize([sample]).to(device)

text

import os
# os.environ['CUDA_VISIBLE_DEVICES']='2'
device = 'cuda' if torch.cuda.is_available() else 'cpu'
# clip_model, compose = clip.load('RN50x4', device = device)
clip_model, compose = clip.load("ViT-B/32", device = device)
text_inputs = (clip.tokenize(data.Text_Transcription.values[321],truncate=True)).to(device)
print(text_inputs)

# image_features = clip_model.encode_image(image)
text_features = clip_model.encode_text(text_inputs)
text_features

data.head(5)

from PIL import ImageFile
from PIL import Image
ImageFile.LOAD_TRUNCATED_IMAGES = True

len(data)

data.img.values[1:10]

def get_data_test(data):
  #data = pd.read_csv(dataset_path)
  text = list(data['text'])
  img_path = list(data['file_name'])
  name = list(data['file_name'])
  # label = list(data['Level1'])
  label = list(data['label'])
  # valence = list(data['Valence'])
  # valence = list(map(lambda x: x - 1 , valence))
  # arousal = list(data['Arousal'])
  # arousal = list(map(lambda x: x - 1 , arousal))
  #optimize memory for features
  text_features,image_features,l,Name,v = [],[],[],[],[]
  # for txt,img,L,A,V in tqdm(zip(text,img_path,label,arousal,valence)):
  for txt,img,L,n in tqdm(zip(text,img_path,label,name)):
    try:
      #img = preprocess(Image.open('/content/drive/.shortcut-targets-by-id/1Z57L19m3ZpJ6bEPdyaIMYuI00Tc2RT1I/memes_our_dataset_hindi/my_meme_data/'+img)).unsqueeze(0).to(device)
      img = Image.open('path/gender_biasness/MAMI_2022_images/test_images/'+img)
    except Exception as e:
      print(e)
      continue

    img = torch.stack([compose(img).to(device)])
    l.append(L)
    Name.append(n)
    # v.append(V)
    #txt = torch.as_tensor(txt)
    with torch.no_grad():
      temp_txt=model.forward(txt, tokenizer).detach().cpu().numpy()
      # temp_txt = model([txt]).detach().cpu().numpy()
      text_features.append(temp_txt)
      temp_img = clip_model.encode_image(img).detach().cpu().numpy()
      image_features.append(temp_img)

      del temp_txt
      del temp_img

      torch.cuda.empty_cache()

    del img
    #del txtla
    torch.cuda.empty_cache()
  return text_features,image_features,l,Name

def get_data(data):
  #data = pd.read_csv(dataset_path)
  text = list(data['Text_Transcription'])
  # text1 = list(data['rel_label1'])
  # text2= list(data['rel_label2'])
  # text3 = list(data['rel_label3'])
  # text4 = list(data['rel_label4'])
  # text5 = list(data['rel_label5'])
  # text6 = list(data['rel_label6'])
  # text7 = list(data['rel_label7'])
  # text8 = list(data['rel_label8'])
  # text9 = list(data['rel_label9'])
  # text10 = list(data['rel_label10'])
  # text11 = list(data['rel_label11'])
  # text12 = list(data['rel_label12'])
  # text13 = list(data['rel_label13'])
  # text14 = list(data['rel_label14'])
  # text15 = list(data['rel_label15'])
  # text16 = list(data['rel_label16'])
  img_path = list(data['file_name'])
  name = list(data['file_name'])
  bias_list = ['woman', 'girl', 'men', 'kitchen', 'man', 'rape', 'time', 'wife', 'sandwich', 'guy', 'fuck', 'bitch', 'meme', 'feminine', 'day', 'thing', 'dishwash', 'year', 'sex', 'center',\
              'feminist', 'friend', 'work', 'date', 'car', 'love', 'job', 'memat', 'boy', 'peopl', 'mom', 'face', 'beer', 'girlfriend', 'world', 'game', 'way', 'life', 'money', 'head', 'eye',\
                  'today', 'hooker', 'husband', 'cougar', 'home', 'maker', 'ladi', 'talk', 'problem']
  # label = list(data['Level1'])
  label = list(data['misogynous'])
  # valence = list(data['Valence'])
  # valence = list(map(lambda x: x - 1 , valence))
  text_features1=[]
  text_features2=[]
  text_features3=[]
  text_features4=[]
  text_features5=[]
  text_features6=[]
  text_features7=[]
  text_features8=[]
  text_features9=[]
  text_features10=[]
  text_features11=[]
  text_features12=[]
  text_features13=[]
  text_features14=[]
  text_features15=[]
  text_features16=[]
  # text
  # text2
  #
  #2
  # rousal = list(data['Arousal'])
  # arousal = list(map(lambda x: x - 1 , arousal))
  #optimize memory for features
  text_features,image_features,l,Name,v, text_list = [],[],[],[],[],[]
  bias_new=[]
  # for txt,img,L,A,V in tqdm(zip(text,img_path,label,arousal,valence)):
  # for txt,txt1,txt2,txt3,txt4,txt5,txt6,txt7,txt8,txt9,txt10,txt11,txt12,txt13,txt14,txt15,txt16,img,L,n,bias_list_check in \
  #   tqdm(zip(text,text1,text2,text3,text4,text5,text6,text7,text8,text9,text10,text11,text12,text13,text14,text15,text16,img_path,label,name,bias_list)):
  for txt,img,L,n in tqdm(zip(text,img_path,label,name)):
    try:
      #img = preprocess(Image.open('/content/drive/.shortcut-targets-by-id/1Z57L19m3ZpJ6bEPdyaIMYuI00Tc2RT1I/memes_our_dataset_hindi/my_meme_data/'+img)).unsqueeze(0).to(device)
      img = Image.open('path/gender_biasness/MAMI_2022_images/training_images/'+img)
    except Exception as e:
      print(e)
      continue

    text_list.append(txt)

    img = torch.stack([compose(img).to(device)])
    l.append(L)
    Name.append(n)
    # bias_new.append(bias_list_check)
    # v.append(V)
    #txt = torch.as_tensor(txt)
    with torch.no_grad():
      temp_txt=model.forward(txt, tokenizer).detach().cpu().numpy()
      # temp_txt1=model.forward(txt1, tokenizer).detach().cpu().numpy()
      # temp_txt2=model.forward(txt2, tokenizer).detach().cpu().numpy()
      # temp_txt3=model.forward(txt3, tokenizer).detach().cpu().numpy()
      # temp_txt4=model.forward(txt4, tokenizer).detach().cpu().numpy()
      # temp_txt5=model.forward(txt5, tokenizer).detach().cpu().numpy()
      # temp_txt6=model.forward(txt6, tokenizer).detach().cpu().numpy()
      # temp_txt7=model.forward(txt7, tokenizer).detach().cpu().numpy()
      # temp_txt8=model.forward(txt8, tokenizer).detach().cpu().numpy()
      # temp_txt9=model.forward(txt9, tokenizer).detach().cpu().numpy()
      # temp_txt10=model.forward(txt10, tokenizer).detach().cpu().numpy()
      # temp_txt11=model.forward(txt11, tokenizer).detach().cpu().numpy()
      # temp_txt12=model.forward(txt12, tokenizer).detach().cpu().numpy()
      # temp_txt13=model.forward(txt13, tokenizer).detach().cpu().numpy()
      # temp_txt14=model.forward(txt14, tokenizer).detach().cpu().numpy()
      # temp_txt15=model.forward(txt15, tokenizer).detach().cpu().numpy()
      # temp_txt16=model.forward(txt16, tokenizer).detach().cpu().numpy()
      # temp_tt = model([txt]).detach().cpu().numpy()
      text_features.append(temp_txt)    #comment for testing purpose
      # text_features1.append(temp_txt1)
      # text_features2.append(temp_txt2)
      # text_features3.append(temp_txt3)
      # text_features4.append(temp_txt4)
      # text_features5.append(temp_txt5)
      # text_features6.append(temp_txt6)
      # text_features7.append(temp_txt7)
      # text_features8.append(temp_txt8)
      # text_features9.append(temp_txt9)
      # text_features10.append(temp_txt10)
      # text_features11.append(temp_txt11)
      # text_features12.append(temp_txt12)
      # text_features13.append(temp_txt13)
      # text_features14.append(temp_txt14)
      # text_features15.append(temp_txt15)
      # text_features16.append(temp_txt16)
      temp_img = clip_model.encode_image(img).detach().cpu().numpy()
      image_features.append(temp_img)

      del temp_txt
      del temp_img

      torch.cuda.empty_cache()

    del img
    #del txtla
    torch.cuda.empty_cache()
  # return text_features,text_features1,text_features2,text_features3,text_features4,text_features5,text_features6,text_features7,text_features8,text_features9,text_features10,text_features11,text_features12,text_features13\
  #   ,text_features14,text_features15,text_features16,image_features,l,Name, bias_new
  return text_features,image_features,l,Name,text_list

# t_f,i_f,label,v,a = get_data(data.head(5))
text_features,image_features,l,Name,text_list = get_data(data.head(5))
# text_features,text_features1,text_features2,text_features3,text_features4,text_features5,text_features6,text_features7,text_features8,text_features9,text_features10,text_features11,text_features12,text_features13\
    # ,text_features14,text_features15,text_features16,image_features,l,Name, bias_list_features = get_data(data.head(5))

text_features[1]

type(text_features)

text_features

len(text_features)

outliers = []
for names in tqdm(list(data['file_name'])):
  #change the path according to your drive
  if not os.path.exists('path/gender_biasness/MAMI_2022_images/training_images/'+names):
    outliers.append(names)

# data = data[~data['Name'].isin(outliers)]

outliers

def get_data_test(data):
  #data = pd.read_csv(dataset_path)
  text = list(data['text'])
  text1 = list(data['rel_label1'])
  text2= list(data['rel_label2'])
  text3 = list(data['rel_label3'])
  text4 = list(data['rel_label4'])
  text5 = list(data['rel_label5'])
  text6 = list(data['rel_label6'])
  text7 = list(data['rel_label7'])
  text8 = list(data['rel_label8'])
  text9 = list(data['rel_label9'])
  text10 = list(data['rel_label10'])
  text11 = list(data['rel_label11'])
  text12 = list(data['rel_label12'])
  text13 = list(data['rel_label13'])
  text14 = list(data['rel_label14'])
  text15 = list(data['rel_label15'])
  text16 = list(data['rel_label16'])
  img_path = list(data['file_name'])
  name = list(data['file_name'])

  # label = list(data['Level1'])
  label = list(data['label'])
  # valence = list(data['Valence'])
  # valence = list(map(lambda x: x - 1 , valence))
  text_features1=[]
  text_features2=[]
  text_features3=[]
  text_features4=[]
  text_features5=[]
  text_features6=[]
  text_features7=[]
  text_features8=[]
  text_features9=[]
  text_features10=[]
  text_features11=[]
  text_features12=[]
  text_features13=[]
  text_features14=[]
  text_features15=[]
  text_features16=[]
  # text
  # text2
  #
  #2
  # rousal = list(data['Arousal'])
  # arousal = list(map(lambda x: x - 1 , arousal))
  #optimize memory for features
  text_features,image_features,l,Name,v = [],[],[],[],[]
  # for txt,img,L,A,V in tqdm(zip(text,img_path,label,arousal,valence)):
  for txt,txt1,txt2,txt3,txt4,txt5,txt6,txt7,txt8,txt9,txt10,txt11,txt12,txt13,txt14,txt15,txt16,img,L,n in \
    tqdm(zip(text,text1,text2,text3,text4,text5,text6,text7,text8,text9,text10,text11,text12,text13,text14,text15,text16,img_path,label,name)):
    try:
      #img = preprocess(Image.open('/content/drive/.shortcut-targets-by-id/1Z57L19m3ZpJ6bEPdyaIMYuI00Tc2RT1I/memes_our_dataset_hindi/my_meme_data/'+img)).unsqueeze(0).to(device)
      img = Image.open('path/gender_biasness/MAMI_2022_images/test_images/'+img)
    except Exception as e:
      print(e)
      continue

    img = torch.stack([compose(img).to(device)])
    l.append(L)
    Name.append(n)
    # v.append(V)
    #txt = torch.as_tensor(txt)
    with torch.no_grad():
      temp_txt=model.forward(txt, tokenizer).detach().cpu().numpy()
      temp_txt1=model.forward(txt1, tokenizer).detach().cpu().numpy()
      temp_txt2=model.forward(txt2, tokenizer).detach().cpu().numpy()
      temp_txt3=model.forward(txt3, tokenizer).detach().cpu().numpy()
      temp_txt4=model.forward(txt4, tokenizer).detach().cpu().numpy()
      temp_txt5=model.forward(txt5, tokenizer).detach().cpu().numpy()
      temp_txt6=model.forward(txt6, tokenizer).detach().cpu().numpy()
      temp_txt7=model.forward(txt7, tokenizer).detach().cpu().numpy()
      temp_txt8=model.forward(txt8, tokenizer).detach().cpu().numpy()
      temp_txt9=model.forward(txt9, tokenizer).detach().cpu().numpy()
      temp_txt10=model.forward(txt10, tokenizer).detach().cpu().numpy()
      temp_txt11=model.forward(txt11, tokenizer).detach().cpu().numpy()
      temp_txt12=model.forward(txt12, tokenizer).detach().cpu().numpy()
      temp_txt13=model.forward(txt13, tokenizer).detach().cpu().numpy()
      temp_txt14=model.forward(txt14, tokenizer).detach().cpu().numpy()
      temp_txt15=model.forward(txt15, tokenizer).detach().cpu().numpy()
      temp_txt16=model.forward(txt16, tokenizer).detach().cpu().numpy()
      # temp_tt = model([txt]).detach().cpu().numpy()
      text_features.append(temp_txt)
      text_features1.append(temp_txt1)
      text_features2.append(temp_txt2)
      text_features3.append(temp_txt3)
      text_features4.append(temp_txt4)
      text_features5.append(temp_txt5)
      text_features6.append(temp_txt6)
      text_features7.append(temp_txt7)
      text_features8.append(temp_txt8)
      text_features9.append(temp_txt9)
      text_features10.append(temp_txt10)
      text_features11.append(temp_txt11)
      text_features12.append(temp_txt12)
      text_features13.append(temp_txt13)
      text_features14.append(temp_txt14)
      text_features15.append(temp_txt15)
      text_features16.append(temp_txt16)
      temp_img = clip_model.encode_image(img).detach().cpu().numpy()
      image_features.append(temp_img)

      del temp_txt
      del temp_img

      torch.cuda.empty_cache()

    del img
    #del txtla
    torch.cuda.empty_cache()
  return text_features,text_features1,text_features2,text_features3,text_features4,text_features5,text_features6,text_features7,text_features8,text_features9,text_features10,text_features11,text_features12,text_features13\
    ,text_features14,text_features15,text_features16,image_features,l,Name

t_f,i_f,label,name, t_f_text = get_data(data.head(5))

type(t_f)

t_f

t_f = np.squeeze(np.asarray(t_f),axis=1)

class HatefulDataset(Dataset):

  def __init__(self,data):

    # self.t_f,self.i_f,self.label,self.v,self.a = get_data(data)
    # self.t_f,self.t_f1,self.t_f2,self.t_f3,self.t_f4,self.t_f5,self.t_f6,self.t_f7,self.t_f8,self.t_f9,self.t_f10,self.t_f11,self.t_f12,self.t_f13,self.t_f14,self.t_f15,\
    #   self.t_f16,self.i_f,self.label,self.name =get_data(data)
    self.t_f,self.i_f,self.label,self.name, self.t_f_text = get_data(data)
    self.t_f = np.squeeze(np.asarray(self.t_f),axis=1)
    # self.t_f1 = np.squeeze(np.asarray(self.t_f1),axis=1)
    # self.t_f2 = np.squeeze(np.asarray(self.t_f2),axis=1)
    # self.t_f3 = np.squeeze(np.asarray(self.t_f3),axis=1)
    # self.t_f4 = np.squeeze(np.asarray(self.t_f4),axis=1)
    # self.t_f5 = np.squeeze(np.asarray(self.t_f5),axis=1)
    # self.t_f6 = np.squeeze(np.asarray(self.t_f6),axis=1)
    # self.t_f7 = np.squeeze(np.asarray(self.t_f7),axis=1)
    # self.t_f8 = np.squeeze(np.asarray(self.t_f8),axis=1)
    # self.t_f9 = np.squeeze(np.asarray(self.t_f9),axis=1)
    # self.t_f10 = np.squeeze(np.asarray(self.t_f10),axis=1)
    # self.t_f11 = np.squeeze(np.asarray(self.t_f11),axis=1)
    # self.t_f12 = np.squeeze(np.asarray(self.t_f12),axis=1)
    # self.t_f13 = np.squeeze(np.asarray(self.t_f13),axis=1)
    # self.t_f14 = np.squeeze(np.asarray(self.t_f14),axis=1)
    # self.t_f15 = np.squeeze(np.asarray(self.t_f15),axis=1)
    # self.t_f16 = np.squeeze(np.asarray(self.t_f16),axis=1)
    self.i_f = np.squeeze(np.asarray(self.i_f),axis=1)



  def __len__(self):
    return len(self.label)

  def __getitem__(self,idx):
    if torch.is_tensor(idx):
      idx = idx.tolist()
    #print(idx)
    name=self.name[idx]
    label = self.label[idx]
    T = self.t_f[idx,:]
    T_text = self.t_f_text[idx]
    # T1 = self.t_f1[idx,:]
    # T2 = self.t_f2[idx,:]
    # T3 = self.t_f3[idx,:]
    # T4 = self.t_f4[idx,:]
    # T5 = self.t_f5[idx,:]
    # T6 = self.t_f6[idx,:]
    # T7 = self.t_f7[idx,:]
    # T8 = self.t_f8[idx,:]
    # T9 = self.t_f9[idx,:]
    # T10 = self.t_f10[idx,:]
    # T11 = self.t_f11[idx,:]
    # T12= self.t_f12[idx,:]
    # T13 = self.t_f13[idx,:]
    # T14 = self.t_f14[idx,:]
    # T15 = self.t_f15[idx,:]
    # T16 = self.t_f16[idx,:]
    I = self.i_f[idx,:]

    # v = self.v[idx]
    # a = self.a[idx]
    sample = {'label':label,'processed_txt':T,'processed_img':I,'name':name, 'text':T_text}
    # sample = {'label':label,'processed_txt':T,'processed_txt1':T1,'processed_txt2':T2,'processed_txt3':T3,'processed_txt4':T4,'processed_txt5':T5,'processed_txt6':T6,'processed_txt7':T7,'processed_txt8':T8,\
              # 'processed_txt9':T9,'processed_txt10':T10,'processed_txt11':T11,'processed_txt12':T12,'processed_txt13':T13,'processed_txt14':T14,'processed_txt15':T15,'processed_txt16':T16,'processed_img':I,'name':name}
    return sample

sample_dataset = HatefulDataset(data.head(100))

sample_dataset[1]

len(data)

torch.save(sample_dataset,'MAMI_unlearning_1000.pt')

!pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd pathCOLING_2023

sample_dataset= torch.load("MAMI_unlearning_1000.pt")

from numpy.linalg import norm
value1,value2,value3,value4,value5,value6,value7,value8,value9,value10,value11,value12,value13,value14,value15,value16,value16,name_list=[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
# compute cosine similarity,
for i in range(len(data)):
    cosine1 = np.dot(sample_dataset_new[i]['processed_txt'],sample_dataset_new[i]['processed_txt1'])/(norm(sample_dataset_new[i]['processed_txt'])*norm(sample_dataset_new[i]['processed_txt1']))
    cosine2 = np.dot(sample_dataset_new[i]['processed_txt'],sample_dataset_new[i]['processed_txt2'])/(norm(sample_dataset_new[i]['processed_txt'])*norm(sample_dataset_new[i]['processed_txt2']))
    cosine3 = np.dot(sample_dataset_new[i]['processed_txt'],sample_dataset_new[i]['processed_txt3'])/(norm(sample_dataset_new[i]['processed_txt'])*norm(sample_dataset_new[i]['processed_txt3']))
    cosine4 = np.dot(sample_dataset_new[i]['processed_txt'],sample_dataset_new[i]['processed_txt4'])/(norm(sample_dataset_new[i]['processed_txt'])*norm(sample_dataset_new[i]['processed_txt4']))
    cosine5 = np.dot(sample_dataset_new[i]['processed_txt'],sample_dataset_new[i]['processed_txt5'])/(norm(sample_dataset_new[i]['processed_txt'])*norm(sample_dataset_new[i]['processed_txt5']))
    cosine6 = np.dot(sample_dataset_new[i]['processed_txt'],sample_dataset_new[i]['processed_txt6'])/(norm(sample_dataset_new[i]['processed_txt'])*norm(sample_dataset_new[i]['processed_txt6']))
    cosine7 = np.dot(sample_dataset_new[i]['processed_txt'],sample_dataset_new[i]['processed_txt7'])/(norm(sample_dataset_new[i]['processed_txt'])*norm(sample_dataset_new[i]['processed_txt7']))
    cosine8 = np.dot(sample_dataset_new[i]['processed_txt'],sample_dataset_new[i]['processed_txt8'])/(norm(sample_dataset_new[i]['processed_txt'])*norm(sample_dataset_new[i]['processed_txt8']))
    cosine9 = np.dot(sample_dataset_new[i]['processed_txt'],sample_dataset_new[i]['processed_txt9'])/(norm(sample_dataset_new[i]['processed_txt'])*norm(sample_dataset_new[i]['processed_txt9']))
    cosine10 = np.dot(sample_dataset_new[i]['processed_txt'],sample_dataset_new[i]['processed_txt10'])/(norm(sample_dataset_new[i]['processed_txt'])*norm(sample_dataset_new[i]['processed_txt10']))
    cosine11 = np.dot(sample_dataset_new[i]['processed_txt'],sample_dataset_new[i]['processed_txt11'])/(norm(sample_dataset_new[i]['processed_txt'])*norm(sample_dataset_new[i]['processed_txt11']))
    cosine12 = np.dot(sample_dataset_new[i]['processed_txt'],sample_dataset_new[i]['processed_txt12'])/(norm(sample_dataset_new[i]['processed_txt'])*norm(sample_dataset_new[i]['processed_txt12']))
    cosine13 = np.dot(sample_dataset_new[i]['processed_txt'],sample_dataset_new[i]['processed_txt13'])/(norm(sample_dataset_new[i]['processed_txt'])*norm(sample_dataset_new[i]['processed_txt13']))
    cosine14 = np.dot(sample_dataset_new[i]['processed_txt'],sample_dataset_new[i]['processed_txt14'])/(norm(sample_dataset_new[i]['processed_txt'])*norm(sample_dataset_new[i]['processed_txt14']))
    cosine15 = np.dot(sample_dataset_new[i]['processed_txt'],sample_dataset_new[i]['processed_txt15'])/(norm(sample_dataset_new[i]['processed_txt'])*norm(sample_dataset_new[i]['processed_txt15']))
    cosine16 = np.dot(sample_dataset_new[i]['processed_txt'],sample_dataset_new[i]['processed_txt16'])/(norm(sample_dataset_new[i]['processed_txt'])*norm(sample_dataset_new[i]['processed_txt16']))
    # print("Cosine Similarity:", cosine)
    name_list.append(sample_dataset_new[i]['name'])
    value1.append(cosine1)
    value2.append(cosine2)
    value3.append(cosine3)
    value4.append(cosine4)
    value5.append(cosine5)
    value6.append(cosine6)
    value7.append(cosine7)
    value8.append(cosine8)
    value9.append(cosine9)
    value10.append(cosine10)
    value11.append(cosine11)
    value12.append(cosine12)
    value13.append(cosine13)
    value14.append(cosine14)
    value15.append(cosine15)
    value16.append(cosine16)

df_test = pd.DataFrame(columns=['name','value1','value2','value3','value4','value5','value6','value7','value8','value9','value10','value11','value12','value13','value14','value15','value16'])
df_test['name']=name_list
df_test['value1']=value1
df_test['value2']=value2
df_test['value3']=value3
df_test['value4']=value4
df_test['value5']=value5
df_test['value6']=value6
df_test['value7']=value7
df_test['value8']=value8
df_test['value9']=value9
df_test['value10']=value10
df_test['value11']=value11
df_test['value12']=value12
df_test['value13']=value13
df_test['value14']=value14
df_test['value15']=value15
df_test['value16']=value16
df_test.to_csv('cosine_similariy_MAMI_train.csv' + ".csv", sep=',')

sample_dataset[2]

# sample_dataset_test[7]

for i in sample_dataset:
  print(i['name'])
  print(i['label'])

import pytorch_lightning as pl

import torch
import torch.nn as nn
import torch.nn.functional as F
class MFB(nn.Module):
    def __init__(self,img_feat_size, ques_feat_size, is_first, MFB_K, MFB_O, DROPOUT_R):
        super(MFB, self).__init__()
        #self.__C = __C
        self.MFB_K = MFB_K
        self.MFB_O = MFB_O
        self.DROPOUT_R = DROPOUT_R

        self.is_first = is_first
        self.proj_i = nn.Linear(img_feat_size, MFB_K * MFB_O)
        self.proj_q = nn.Linear(ques_feat_size, MFB_K * MFB_O)

        self.dropout = nn.Dropout(DROPOUT_R)
        self.pool = nn.AvgPool1d(MFB_K, stride = MFB_K)

    def forward(self, img_feat, ques_feat, exp_in=1):
        '''
            img_feat.size() -> (N, C, img_feat_size)    C = 1 or 100
            ques_feat.size() -> (N, 1, ques_feat_size)
            z.size() -> (N, C, MFB_O)
            exp_out.size() -> (N, C, K*O)
        '''
        batch_size = img_feat.shape[0]
        img_feat = self.proj_i(img_feat)                # (N, C, K*O)
        ques_feat = self.proj_q(ques_feat)              # (N, 1, K*O)

        exp_out = img_feat * ques_feat             # (N, C, K*O)
        exp_out = self.dropout(exp_out) if self.is_first else self.dropout(exp_out * exp_in)     # (N, C, K*O)
        z = self.pool(exp_out) * self.MFB_K         # (N, C, O)
        z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))
        z = F.normalize(z.view(batch_size, -1))         # (N, C*O)
        z = z.view(batch_size, -1, self.MFB_O)      # (N, C, O)
        return z

data = data[~data['Name'].isin(outliers)]

!pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd pathCOLING_2023/

len(sample_dataset)

torch.manual_seed(123)
t_p,v_p = torch.utils.data.random_split(sample_dataset,[700,300])

# torch.manual_seed(123)
t_p,te_p = torch.utils.data.random_split(t_p,[500,200])

t_p[1]['text']

# outliers = ['ravan38.png',
# 'ravan283.png',
# 'ravan26.png',
# 'ravan283.png',
# 'ravan282.png',
# 'rel563.png',
# 'ravan341.png',
# 'politics363.png',
# 'ravan26.png',
# 'ravan284.png',
# 'hin32.png',
# 'ravan299.png',
# 'ravan296.png',
# 'ravan25.png',
# 'file_new_426.png',
# 'm_50.png',
# 'test31.png',
# 'ravan24.png',
# 'rel563.png',
# 'kalam360.png',
# 'chaukidar43.png',
# 'chaukidar37.png',
# 'ravan23.png',
# 'ravan344.png',
# 'ravan283.png',
# 'gandhiji77.png',
# 'test25.png',
# 'ravan342.png',
# 'ravan38.png',
# 'mix_meme7.png',
# 'chaukidar37.png',
# 'match251.png',
# 'file_new_5.png',
# 'ravan284.png',
# 'chaukidar37.png',
# 'ravan284.png',
# 'ravan26.png',
# 'rel563.png',
# 'ravan2.png',
# 'ravan343.png',
# 'ravan20.png',
# 'ravan38.png']

# filtering out the rows which contain the outlier images
data = data[~data['Name'].isin(outliers)]

bias_words= ['WOMAN', 'girls', 'men', 'kitchen', 'man', 'rape', 'time', 'wife', 'sandwich', 'guy', 'fuck', 'bitch', 'meme', 'feminine', 'day', 'thing', 'dishwash', 'year', 'sex', 'center',\
              'feminist', 'friend', 'work', 'date', 'car', 'love', 'job', 'memat', 'boy', 'peopl', 'mom', 'face', 'beer', 'girlfriend', 'world', 'game', 'way', 'life', 'money', 'head', 'eye',\
                  'today', 'hooker', 'husband', 'cougar', 'home', 'maker', 'ladi', 'talk', 'problem']
text_list= ['Nobody: Girls on Instagram: time D yallgayof ifunny.co', "when feminist WOMAN memat realise it's called feminist rape mankind feminist not womankind Small Medium OOF SIZE Large", \
 "You're dating somebody's Ex Somebody is money dating your ex Your Ex is getting somebody's Ex. mom In this life", 'Top 3 Whale jump captured by Photographer amyschumer 8h Amy Schumer / Instagram Oh my GOD', \
    "THEY CALL ME MR LIGHTNING Lightning feminine never strikes in the same place twice. That's OK I'm happy to bitch strike just once. \\o/ Motivated thing Photos.com", \
        '73 R "DON\'T TRY TO TURN A HOE INTO beer A HOUSEWIFE"-COLONEL CHARLES BECKWITH', 'destroying the pussy damaging the female girlfriend reproductive organ beyond repair']
print(len(text_list))

texts = [
    ['Three blind horses listening to Mozart.'],
    ['Älgen är skogens konung!'],
    ['Wie leben Eisbären in der Antarktis?'],
    ['Вы знали, что все белые медведи левши?']
]
model_name = 'M-CLIP/XLM-Roberta-Large-Vit-L-14'
# Load Model & Tokenizer
model = pt_multilingual_clip.MultilingualCLIP.from_pretrained(model_name)
tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)
embeddings = model.forward(texts, tokenizer)
print(embeddings.shape)

biased_word_list = []
text_features=[]
 # Create a list to store biased words
for sample_text in text_list:
    words = sample_text.split()  # Split the text into words
    biased_words_in_sample = []  # Create a list to store biased words in this sample
    for word in words:
        if word in bias_words:  # Convert to uppercase to match bias_words
            if word not in biased_words_in_sample:
                biased_words_in_sample.append(word)
    # if biased_words_in_sample:
    #     print(sample_text)  # Print the sample text with biased words
    biased_word_list.append(biased_words_in_sample)
    output_list = [' '.join(sublist) if sublist else '[SEP]' for sublist in biased_word_list]
    # text_features=[]
    print(output_list)
    temp_txt=model.forward(output_list, tokenizer).detach().cpu().numpy()
    # print(temp_txt)
    # text_features.append(temp_txt)
    # nested_list = [['time'], ['feminist', 'WOMAN', 'memat', 'rape'], ['money', 'mom', 'life'], [], ['feminine', 'bitch', 'thing'], ['beer'], ['girlfriend']]

    # flat_list = [word for sublist in biased_word_list for word in sublist]
    # flat_list = [word for word in biased_word_list]
print(biased_word_list)


# print("t_f.shape", t_f.shape)

type(temp_txt)

temp_txt[4]

len(text_features)

tf, vf,l,n=

t_f = np.asarray(temp_txt)
# t_f = np.squeeze(np.asarray(temp_txt),axis=-1)

t_f

t_f.shape

t_f[3]

# input_list = [['time'], ['feminist', 'WOMAN', 'memat', 'rape'], ['money', 'mom', 'life'], [], ['feminine', 'bitch', 'thing'], ['beer'], ['girlfriend']]

output_list = [' '.join(sublist) if sublist else '\B' for sublist in biased_word_list]
text_features=[]
temp_txt=model.forward(output_list, tokenizer).detach().cpu().numpy()
# print(temp_txt)
text_features.append(temp_txt)
# print(text_features)
# print(len(text_features)) #comment for testing purpose
# T = t_f[idx,:]
# t_f = np.squeeze(np.asarray(temp_txt),axis=1)
t_f = np.asarray(temp_txt)
print(output_list)

t_f

type(t_f)

len(data)

def calculate_mi_text(z, x):
    # Calculate the joint probability of z and x.
    p_xz = torch.mm(z.t(), x)

    # Calculate the marginal probability of z.
    p_z = torch.sum(p_xz, dim=1)

    # Calculate the marginal probability of x.
    p_x = torch.sum(p_xz, dim=0)

    # Calculate the mutual information between z and x.
    # mi_text = torch.sum(p_xz * torch.log(p_xz / p_z * p_x))

    mi_text = torch.sum(p_xz * torch.log((p_xz / (p_z.unsqueeze(1) * p_x)).clamp(min=1e-10)))

    return mi_text

texts = [
    ['Three blind horses listening to Mozart.'],
    ['Älgen är skogens konung!'],
    ['Wie leben Eisbären in der Antarktis?'],
    ['Вы знали, что все белые медведи левши?']
]
model_name = 'M-CLIP/XLM-Roberta-Large-Vit-L-14'
# Load Model & Tokenizer
model = pt_multilingual_clip.MultilingualCLIP.from_pretrained(model_name)
tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)
embeddings = model.forward(texts, tokenizer)

embeddings.shape

import scipy.stats as stats
def calculate_mutual_information(t_f_list, z_new_list, min_value=0):
  """
  Calculates the mutual information between each sample in the batch of t_f_list and z_new.

  Args:
    t_f_list: A list of 128 vectors of size 768.
    z_new_list: A list of 128 vectors of size 768.
    min_value: The minimum value that the entropy can be.

  Returns:
    A list of 128 mutual information values.
  """

  mutual_information_list = []
  for t_f, z_new in zip(t_f_list, z_new_list):
    mutual_information = stats.entropy(t_f, z_new)
    mutual_information_list.append(mutual_information)

  return mutual_information_list


t_f_list = torch.rand(128, 768)
z_new_list = torch.rand(128, 768)

mutual_information_list = calculate_mutual_information(t_f_list, z_new_list)

print(mutual_information_list)
print(len(mutual_information_list))

len(mutual_information_list)

# Inside your LightningModule class
def calculate_MI(t_f_list, z_new):
    # Calculate probabilities p(x, y), p(x), p(y)
    joint_prob = torch.zeros((t_f_list.shape[1], z_new.shape[1]), dtype=torch.float32)
    p_x = torch.sum(t_f_list, dim=0) / t_f_list.shape[0]
    p_y = torch.sum(z_new, dim=0) / z_new.shape[0]

    # Calculate joint probability
    for i in range(t_f_list.shape[1]):
        for j in range(z_new.shape[1]):
            joint_prob[i, j] = torch.sum(t_f_list[:, i] * z_new[:, j]) / t_f_list.shape[0]

    # Calculate mutual information using the formula
    mutual_information = 0.0
    for i in range(t_f_list.shape[1]):
        for j in range(z_new.shape[1]):
            if joint_prob[i, j] > 0.0:  # Avoid log(0)
                mutual_information += joint_prob[i, j] * torch.log(joint_prob[i, j] / (p_x[i] * p_y[j]))

    return mutual_information

import torch
import math
from torch import nn
import pytorch_lightning as pl
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score,precision_score
from torch.utils.data import DataLoader, random_split
from pytorch_lightning.callbacks import ModelCheckpoint
from torch.nn import functional as F
from torchvision.datasets import MNIST
from torchvision import datasets, transforms
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
class Classifier(pl.LightningModule):
  def __init__(self):
    super().__init__()
    self.MFB = MFB(768,768,True,256,256,0.1)
    # self.fin_y_shape = torch.nn.Linear(768,512)
    self.fin_text_list = torch.nn.Linear(768,256)
    self.fin_text_feat = torch.nn.Linear(768,256)
    self.fin_old = torch.nn.Linear(256,2)
    self.fin = torch.nn.Linear(16 * 768, 64)
  def calculate_mutual_information(self, t_f_list, z_new):
    # Convert NumPy arrays to PyTorch tensors
    t_f_tensor = torch.from_numpy(t_f_list).to(self.device)
    z_new_tensor = torch.from_numpy(z_new).to(self.device)

    # Calculate probabilities p(x, y), p(x), p(y)
    joint_prob = torch.zeros((t_f_tensor.shape[1], z_new_tensor.shape[1]), dtype=torch.float32, device=self.device)
    p_x = torch.sum(t_f_tensor, dim=0) / t_f_tensor.shape[0]
    p_y = torch.sum(z_new_tensor, dim=0) / z_new_tensor.shape[0]

    # Add a small epsilon to avoid logarithm of zero
    eps = 1e-10
    joint_prob = joint_prob + eps
    p_x = p_x + eps
    p_y = p_y + eps

    # Calculate joint probability
    for i in range(t_f_tensor.shape[1]):
        for j in range(z_new_tensor.shape[1]):
            joint_prob[i, j] = torch.sum(t_f_tensor[:, i] * z_new_tensor[:, j]) / t_f_tensor.shape[0]

    # Calculate mutual information using the formula
    mutual_information = 0.0
    for i in range(t_f_tensor.shape[1]):
        for j in range(z_new_tensor.shape[1]):
            mutual_information += joint_prob[i, j] * torch.log(joint_prob[i, j] / (p_x[i] * p_y[j]))

    return mutual_information
  def forward(self, x,y, text_list ):
      x_,y_ = x,y
      bias_words= ['WOMAN', 'girl', 'men', 'kitchen', 'man', 'rape', 'time', 'wife', 'sandwich', 'guy', 'fuck', 'bitch', 'meme', 'feminine', 'day', 'thing', 'dishwash', 'year', 'sex', 'center',\
              'feminist', 'friend', 'work', 'date', 'car', 'love', 'job', 'memat', 'boy', 'people', 'mom', 'face', 'beer', 'girlfriend', 'world', 'game', 'way', 'life', 'money', 'head', 'eye',\
                  'today', 'hooker', 'husband', 'cougar', 'home', 'maker', 'ladi', 'talk', 'problem']
      object_list=['kitchen', 'apron', 'heels', 'makeup', 'dress', 'vacuum', 'sandwich', 'stove', 'laundry', 'dishes', 'broom', 'mop', 'lipstick', 'nail_polish', 'skirt', 'high_heels', 'oven',\
                    'perfume', 'curling_iron', 'shopping_bag', 'cleaning_spray', 'sponge', 'fridge', 'washing_machine', 'iron', 'blender', 'toaster', 'cookbook', 'spatula', 'mixer', 'rolling_pin', \
                      'cake', 'salad', 'soup', 'pan', 'pot', 'mirror', 'hair_dryer', 'brush', 'comb', 'eyeshadow', 'mascara', 'foundation', 'blush', 'eyeliner', 'handbag', 'jewelry', 'scarf', 'gloves', 'hat']

      z = self.MFB(torch.unsqueeze(y, axis=1), torch.unsqueeze(x, axis=1))
      z_new = torch.squeeze(z, dim=1)
      c = self.fin_old(z_new)
      # print("z_new.shape",z_new.shape)
      output = torch.log_softmax(c, dim=1)
      biased_word_list = []  # Create a list to store biased words
      for sample_text in text_list:
        words = sample_text.split()  # Split the text into words
        biased_words_in_sample = []  # Create a list to store biased words in this sample
        for word in words:
            if word in bias_words:
               if word not in biased_words_in_sample:
                biased_words_in_sample.append(word)
                # print(biased_words_in_sample)# Convert to uppercase to match bias_words
        biased_word_list.append(biased_words_in_sample)
      output_list = [' '.join(sublist) if sublist else '[SEP]' for sublist in biased_word_list]
      temp_txt=model.forward(output_list, tokenizer).detach().cpu().numpy()
      # print("temp_txt",temp_txt)
      t_f = np.asarray(temp_txt)
      t_f_tensor = torch.from_numpy(t_f).cuda()
      t_f_list= self.fin_text_list(t_f_tensor)
      t_f_list= t_f_list.detach().cpu().numpy()
      print(t_f_list.shape)
      z_new= z_new.detach().cpu().numpy()
      # mi_mm_biased= calculate_MI(t_f_list,z_new)
      # cosine_sim = F.cosine_similarity(torch.from_numpy(t_f_list).to(self.device), z_new, dim=1)
      # print("Cosine Similarity Shape:", cosine_sim.shape)
          # Calculate the cosine similarity between t_f_list and z_new
      # z_new_tensor = torch.from_numpy(z_new.cpu().numpy()).to(self.device)

      # cosine_sim = F.cosine_similarity(torch.from_numpy(t_f_list).to(self.device), z_new_tensor, dim=1)
      cosine_sim = F.cosine_similarity(torch.from_numpy(t_f_list).to(self.device), z_new, dim=1)

      biased_object_list = []
      for sample_object in object_list:
            objects = sample_object.split()
            biased_objects_in_sample = [obj for obj in objects if obj in object_list]
            biased_object_list.append(biased_objects_in_sample)

      output_object_list = [' '.join(sublist) if sublist else '[SEP]' for sublist in biased_object_list]
      temp_obj = model.forward(output_object_list, tokenizer).detach().cpu().numpy()
      o_f = np.asarray(temp_obj)
      o_f_tensor = torch.from_numpy(o_f).cuda()
      o_f_list = self.fin_text_list(o_f_tensor)

      mutual_information_words = self.calculate_mutual_information(t_f_list, z_new)
      mutual_information_objects = self.calculate_mutual_information(o_f_list, z_new)
      print("Mutual Information (Words):", mutual_information_words)
      print("Mutual Information (Objects):", mutual_information_objects)
      print("Cosine Similarity Shape:", cosine_sim.shape)
      # mutual_information = self.calculate_mutual_information(t_f_list,z_new)
      # print("mutual_information.shape",mutual_information)
      t_feat= self.fin_text_feat(x)
      return output

  def cross_entropy_loss(self, logits, labels):
    return F.nll_loss(logits, labels)

  def training_step(self, train_batch, batch_idx):
      lab,txt,img,name, text_list= train_batch


      lab = train_batch[lab]
      #print(lab)
      txt = train_batch[txt]
      text_list = train_batch[text_list]
      img = train_batch[img]
    #   logit_offen= self.forward(txt,img,e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12, e13,e14, e15,e16)
      logit_offen= self.forward(txt,img,text_list)
      loss = self.cross_entropy_loss(logit_offen, lab)
      self.log('train_loss', loss)
      return loss


  def validation_step(self, val_batch, batch_idx):
      lab,txt,img,name , text_list= val_batch
    #   lab,txt,e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12, e13,e14, e15,e16,img,name= val_batch

      lab = val_batch[lab]
      #print(lab)
      text_list = val_batch[text_list]
      txt = val_batch[txt]
      img = val_batch[img]
      logits= self.forward(txt,img, text_list)
      tmp = np.argmax(logits.detach().cpu().numpy(),axis=-1)
      loss = self.cross_entropy_loss(logits, lab)
      lab = lab.detach().cpu().numpy()
      self.log('val_acc', accuracy_score(lab,tmp))
      self.log('val_roc_auc',roc_auc_score(lab,tmp))
      self.log('val_loss', loss)
      tqdm_dict = {'val_acc': accuracy_score(lab,tmp)}
      return {
                'progress_bar': tqdm_dict,
      'val_f1 offensive': f1_score(lab,tmp,average='macro')
      }

  def validation_epoch_end(self, validation_step_outputs):
    outs = []
    outs1,outs2,outs3,outs4,outs5,outs6,outs7,outs8,outs9,outs10,outs11,outs12,outs13,outs14,outs16,outs17 = \
    [],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
    for out in validation_step_outputs:
      outs.append(out['progress_bar']['val_acc'])
      outs14.append(out['val_f1 offensive'])
    self.log('val_acc_all_offn', sum(outs)/len(outs))
    self.log('val_f1 offensive', sum(outs14)/len(outs14))
    print(f'***val_acc_all_offn at epoch end {sum(outs)/len(outs)}****')
    print(f'***val_f1 offensive at epoch end {sum(outs14)/len(outs14)}****')

  def test_step(self, batch, batch_idx):
    #   lab,txt,e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12, e13,e14, e15,e16,img,name= batch
      lab,txt,img,name= batch

      lab = batch[lab]
      #print(lab)
      txt = batch[txt]
      img = batch[img]
      text_list = batch[text_list]
      logits= self.forward(txt,img, text_list)
      tmp = np.argmax(logits.detach().cpu().numpy(),axis=-1)
      loss = self.cross_entropy_loss(logits, lab)
      lab = lab.detach().cpu().numpy()
      self.log('test_acc', accuracy_score(lab,tmp))
      self.log('test_roc_auc',roc_auc_score(lab,tmp))
      self.log('test_loss', loss)
      tqdm_dict = {'test_acc': accuracy_score(lab,tmp)}
      #print('Val acc {}'.format(accuracy_score(lab,tmp)))
      return {
                'progress_bar': tqdm_dict,
                'test_acc': accuracy_score(lab,tmp),
                'test_f1_score': f1_score(lab,tmp,average='macro'),
      }
  def test_epoch_end(self, outputs):
      # OPTIONAL
      outs = []
      outs1,outs2,outs3,outs4,outs5,outs6,outs7,outs8,outs9,outs10,outs11,outs12,outs13,outs14 = \
      [],[],[],[],[],[],[],[],[],[],[],[],[],[]
      for out in outputs:
        # outs15.append(out['test_loss_target'])
        outs.append(out['test_acc'])
        outs2.append(out['test_f1_score'])
      self.log('test_acc', sum(outs)/len(outs))
      self.log('test_f1_score', sum(outs2)/len(outs2))

  def configure_optimizers(self):
    # optimizer = torch.optim.Adam(self.parameters(), lr=3e-2)
    optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)

    return optimizer


class HmDataModule(pl.LightningDataModule):

  def setup(self, stage):
    self.hm_train = t_p
    self.hm_val = v_p
    # self.hm_test = test
    self.hm_test = te_p

  def train_dataloader(self):
    return DataLoader(self.hm_train, batch_size=128, drop_last=True)

  def val_dataloader(self):
    return DataLoader(self.hm_val, batch_size=128, drop_last=True)

  def test_dataloader(self):
    return DataLoader(self.hm_test, batch_size=128, drop_last=True)

data_module = HmDataModule()
checkpoint_callback = ModelCheckpoint(
     monitor='val_acc_all_offn',
     dirpath='ckpt_unlearning/',
     filename='epoch{epoch:02d}-val_f1_all_offn{val_acc_all_offn:.2f}',
     auto_insert_metric_name=False,
     save_top_k=1,
    mode="max",
 )
all_callbacks = []
all_callbacks.append(checkpoint_callback)
# train
from pytorch_lightning import seed_everything
seed_everything(42, workers=True)
hm_model = Classifier()
gpus = 1 if torch.cuda.is_available() else 0
trainer = pl.Trainer(gpus=gpus,deterministic=True,max_epochs=2,precision=16,callbacks=all_callbacks)
# trainer = pl.Trainer(gpus=gpus,max_epochs=10,callbacks=all_callbacks)
trainer.fit(hm_model, data_module)

import torch
import torch.nn as nn

class BiLSTMAttention(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(BiLSTMAttention, self).__init__()
        self.bilstm = torch.nn.LSTM(input_dim, hidden_dim, bidirectional=True)
        self.linear_layer = torch.nn.Linear(hidden_dim * 2, hidden_dim)
        self.attention = torch.nn.Linear(hidden_dim*2, 1)
        # self.output_layer = nn.Linear(hidden_dim + input_dim, output_dim)
        self.output_layer = torch.nn.Linear(832, 64)


    def forward(self, x, additional_tensor):
        # Pass input tensor through BiLSTM
        lstm_output, _ = self.bilstm(x)

        # Calculate attention weights
        attention_weights = self.attention(lstm_output)
        attention_weights = F.softmax(attention_weights, dim=0)

        # Apply attention to BiLSTM output
        attentive_representation = torch.sum(attention_weights * lstm_output, dim=0)

        # Apply linear layer to BiLSTM output
        lstm_output = self.linear_layer(lstm_output)

        # Squeeze dimensions
        lstm_output = lstm_output.squeeze(1)
        attentive_representation = attentive_representation.squeeze(1)

        # Concatenate attentive representation with additional tensor
        concatenated_output = torch.cat((lstm_output, attentive_representation.expand(lstm_output.size(0), -1)), dim=1)
        # print("concatenated_output",concatenated_output.shape)
                # Add the additional tensor
        # print(concatenated_output)
        # print("addtional tensor",additional_tensor)
        # print('additional_tensor',additional_tensor.shape)
        # concatenated_output += additional_tensor
        concatenated = torch.cat([concatenated_output,additional_tensor], dim=1)

        # print(concatenated)
        # print("concatenated",concatenated.shape)
        # test=concatenated_output.unsqueeze(1)
        # print("test", test.shape)
        # Pass concatenated output through the output layer
        output = self.output_layer(concatenated)
        # output = self.output_layer(concatenated_output.unsqueeze(1))
        # print("output.shape",output.shape)
        return output


# Example usage
input_dim = 768
hidden_dim = 256
output_dim = 2
batch_size = 128

# Create dummy tensors
x = torch.randn(batch_size, 1, input_dim)
additional_tensor = torch.randn(batch_size, 64)

# Create the BiLSTM Attention model
model = BiLSTMAttention(input_dim, hidden_dim, output_dim)

# Forward pass
output = model(x, additional_tensor)

print(output.shape)  # Shape: (128, 1, 2)

class Classifier(pl.LightningModule):

  def __init__(self):
    super().__init__()
    self.MFB = MFB(512,768,True,256,64,0.1)
    self.model = BiLSTMAttention(input_dim=768, hidden_dim=256, output_dim=64)

    self.fin_old = torch.nn.Linear(64*3,2)
    self.fin = torch.nn.Linear(16 * 768, 64)

  # def forward(self, x,y,v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15,v16, bias_list):
  def forward(self, x,y,v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15,v16):
      x_,y_ = x,y
      z = self.MFB(torch.unsqueeze(y,axis=1),torch.unsqueeze(x,axis=1))
      vectors = [v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16]
      cosine_similarities = [torch.cosine_similarity(vector, x, dim=0) for vector in vectors]
      weights = F.softmax(torch.cat(cosine_similarities, dim=0), dim=0)
      weighted_vectors = [weight * vector for weight, vector in zip(weights, vectors)]
      concatenated_vector = torch.cat(weighted_vectors, dim=1)
      # Flatten the concatenated vector
      flattened_vector = concatenated_vector.view(concatenated_vector.size(0), -1)
      output = self.fin(flattened_vector)
      z_new=torch.squeeze(z,dim=1)
      memory = self.model(torch.unsqueeze(x,axis=1), z_new)
      concat_new = torch.cat([output,z_new,memory], dim=1)
      c = self.fin_old(concat_new)
      output = torch.log_softmax(c, dim=1)
      return output,z_new

  def cross_entropy_loss(self, logits, labels):
    return F.nll_loss(logits, labels)

  def training_step(self, train_batch, batch_idx):
      # lab,txt,img,v,a,_,_,e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12 = train_batch
      lab,txt,e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12, e13,e14, e15,e16,img,name= train_batch

      lab = train_batch[lab]
      #print(lab)
      txt = train_batch[txt]
      e1 = train_batch[e1]
      e2 = train_batch[e2]
      e3 = train_batch[e3]
      e4 = train_batch[e4]
      e5 = train_batch[e5]
      e6 = train_batch[e6]
      e7 = train_batch[e7]
      e8 = train_batch[e8]
      e9 = train_batch[e9]
      e10 = train_batch[e10]
      e11 = train_batch[e11]
      e12 = train_batch[e12]
      e13 = train_batch[e13]
      e14 = train_batch[e14]
      e15 = train_batch[e15]
      e16 = train_batch[e16]
      #4rint(txt4
      img = train_batch[img]
      logit_offen,vec_teach= self.forward(txt,img,e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12, e13,e14, e15,e16)
      loss = self.cross_entropy_loss(logit_offen, lab)
      self.log('train_loss', loss)
      return loss


  def validation_step(self, val_batch, batch_idx):
      # lab,txt,img,name= val_batch
      lab,txt,e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12, e13,e14, e15,e16,img,name= val_batch

      lab = val_batch[lab]
      #print(lab)
      txt = val_batch[txt]
      e1 = val_batch[e1]
      e2 = val_batch[e2]
      e3 = val_batch[e3]
      e4 = val_batch[e4]
      e5 = val_batch[e5]
      e6 = val_batch[e6]
      e7 = val_batch[e7]
      e8 = val_batch[e8]
      e9 = val_batch[e9]
      e10 = val_batch[e10]
      e11 = val_batch[e11]
      e12 = val_batch[e12]
      e13 = val_batch[e13]
      e14 = val_batch[e14]
      e15 = val_batch[e15]
      e16 = val_batch[e16]
      #4rint(txt4
      img = val_batch[img]
      logits,vec_teach= self.forward(txt,img,e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12, e13,e14, e15,e16)
      tmp = np.argmax(logits.detach().cpu().numpy(),axis=-1)
      loss = self.cross_entropy_loss(logits, lab)
      lab = lab.detach().cpu().numpy()
      self.log('val_acc', accuracy_score(lab,tmp))
      self.log('val_roc_auc',roc_auc_score(lab,tmp))
      self.log('val_loss', loss)
      tqdm_dict = {'val_acc': accuracy_score(lab,tmp)}
      return {
                'progress_bar': tqdm_dict,
      'val_f1 offensive': f1_score(lab,tmp,average='macro')
      }

  def validation_epoch_end(self, validation_step_outputs):
    outs = []
    outs1,outs2,outs3,outs4,outs5,outs6,outs7,outs8,outs9,outs10,outs11,outs12,outs13,outs14,outs16,outs17 = \
    [],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
    for out in validation_step_outputs:
      outs.append(out['progress_bar']['val_acc'])
      outs14.append(out['val_f1 offensive'])
    self.log('val_acc_all_offn', sum(outs)/len(outs))
    self.log('val_f1 offensive', sum(outs14)/len(outs14))
    print(f'***val_acc_all_offn at epoch end {sum(outs)/len(outs)}****')
    print(f'***val_f1 offensive at epoch end {sum(outs14)/len(outs14)}****')

  def test_step(self, batch, batch_idx):
      lab,txt,e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12, e13,e14, e15,e16,img,name= batch

      lab = batch[lab]
      #print(lab)
      txt = batch[txt]
      e1 = batch[e1]
      e2 = batch[e2]
      e3 = batch[e3]
      e4 = batch[e4]
      e5 = batch[e5]
      e6 = batch[e6]
      e7 = batch[e7]
      e8 = batch[e8]
      e9 = batch[e9]
      e10 = batch[e10]
      e11 = batch[e11]
      e12 = batch[e12]
      e13 = batch[e13]
      e14 = batch[e14]
      e15 = batch[e15]
      e16 = batch[e16]
      #4rint(txt4
      img = batch[img]
      logits,vec_teach= self.forward(txt,img,e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12, e13,e14, e15,e16)
      tmp = np.argmax(logits.detach().cpu().numpy(),axis=-1)
      loss = self.cross_entropy_loss(logits, lab)
      lab = lab.detach().cpu().numpy()
      self.log('test_acc', accuracy_score(lab,tmp))
      self.log('test_roc_auc',roc_auc_score(lab,tmp))
      self.log('test_loss', loss)
      tqdm_dict = {'test_acc': accuracy_score(lab,tmp)}
      #print('Val acc {}'.format(accuracy_score(lab,tmp)))
      return {
                'progress_bar': tqdm_dict,
                'test_acc': accuracy_score(lab,tmp),
                'test_f1_score': f1_score(lab,tmp,average='macro'),
      }
  def test_epoch_end(self, outputs):
      # OPTIONAL
      outs = []
      outs1,outs2,outs3,outs4,outs5,outs6,outs7,outs8,outs9,outs10,outs11,outs12,outs13,outs14 = \
      [],[],[],[],[],[],[],[],[],[],[],[],[],[]
      for out in outputs:
        # outs15.append(out['test_loss_target'])
        outs.append(out['test_acc'])
        outs2.append(out['test_f1_score'])
      self.log('test_acc', sum(outs)/len(outs))
      self.log('test_f1_score', sum(outs2)/len(outs2))

  def configure_optimizers(self):
    # optimizer = torch.optim.Adam(self.parameters(), lr=3e-2)
    optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)

    return optimizer


class HmDataModule(pl.LightningDataModule):

  def setup(self, stage):
    self.hm_train = t_p
    self.hm_val = v_p
    # self.hm_test = test
    self.hm_test = te_p

  def train_dataloader(self):
    return DataLoader(self.hm_train, batch_size=128, drop_last=True)

  def val_dataloader(self):
    return DataLoader(self.hm_val, batch_size=128, drop_last=True)

  def test_dataloader(self):
    return DataLoader(self.hm_test, batch_size=128, drop_last=True)

data_module = HmDataModule()
checkpoint_callback = ModelCheckpoint(
     monitor='val_acc_all_offn',
     dirpath='ckpt_unlearning/',
     filename='epoch{epoch:02d}-val_f1_all_offn{val_acc_all_offn:.2f}',
     auto_insert_metric_name=False,
     save_top_k=1,
    mode="max",
 )
all_callbacks = []
all_callbacks.append(checkpoint_callback)
# train
from pytorch_lightning import seed_everything
seed_everything(42, workers=True)
hm_model = Classifier()
gpus = 1 if torch.cuda.is_available() else 0
trainer = pl.Trainer(gpus=gpus,deterministic=True,max_epochs=60,precision=16,callbacks=all_callbacks)
# trainer = pl.Trainer(gpus=gpus,max_epochs=10,callbacks=all_callbacks)
trainer.fit(hm_model, data_module)

!pwd

word_prob_pairs = [('woman', 1126), ('girl', 474), ('men', 287), ('kitchen', 166), ('man', 135), ('rape', 135), ('time', 113), ('wife', 109), ('sandwich', 109), ('guy', 105), ('fuck', 102), ('bitch', 98), ('meme', 96), ('femin', 90), ('day', 90), ('thing', 76), ('dishwash', 71), ('year', 68), ('sex', 67), ('center', 63), ('feminist', 63), ('friend', 57), ('work', 54), ('date', 54), ('car', 52), ('love', 52), ('job', 51), ('memat', 50), ('boy', 49), ('peopl', 49), ('mom', 46), ('face', 45), ('beer', 44), ('girlfriend', 43), ('world', 42), ('game', 42), ('way', 41), ('life', 40), ('money', 38), ('head', 37), ('eye', 37), ('today', 36), ('hooker', 35), ('husband', 34), ('cougar', 32), ('home', 32), ('maker', 30), ('ladi', 30), ('talk', 29), ('problem', 27)]

# Separate words and probabilities into two lists
words_list = [pair[0] for pair in word_prob_pairs]
probabilities_list = [pair[1] for pair in word_prob_pairs]

print("Words List:", words_list)
print("Probabilities List:", probabilities_list)

!pwd

test_dataloader = DataLoader(dataset=te_p, batch_size=1478)
ckpt_path = 'path/gender_biasness/ckpt_unlearning/epoch_final.ckpt' # put ckpt_path according to the path output in the previous cell
trainer.test(dataloaders=test_dataloader,ckpt_path=ckpt_path)

